{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Creating Cutting-Edge Geodemographic Classifications from Scratch in Python\"\n",
    "author: \"Owen Goodwin\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-location: left\n",
    "    toc-float: true\n",
    "    theme: cosmo\n",
    "    highlight-style: github\n",
    "    reference-location: margin\n",
    "    citation-location: margin\n",
    "    margin-references: true\n",
    "\n",
    "bibliography: ./bibliography.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Content\n",
    "\n",
    "This notebook contains the full workflow for producing a geodemographic classification from scratch in python using k-means clustering. \n",
    "\n",
    "* **Data Access and Processing:**\n",
    "    * Access UK census data and process using Pandas.\n",
    "    * Select a specific region of interest (e.g., Liverpool City Region, Greater Manchester, Greater London).\n",
    "\n",
    "* **Census Data Analysis and Variable selection:**\n",
    "    * Select relevant census variables for clustering.\n",
    "    * Standardise variables.\n",
    "    * Perform correlation & variance analysis to identify potentially redundant variables.\n",
    "    * Alternative variable selection methods (e.g., PCA, Autoencoders).\n",
    "\n",
    "* **Clustering:**\n",
    "    * Determine optimal number of clusters using Clustergrams.\n",
    "    * Apply K-Means clustering to classify areas based on selected variables.\n",
    "    * Perform top-down hierarchical clustering to divide clusters into subgroups.\n",
    "    \n",
    "* **Analytical Techniques:**\n",
    "    * Use UMAP (Uniform Manifold Approximation and Projection) to visualise high-dimensional embeddings in 2D.\n",
    "\n",
    "* **Visualisation and Communication:**\n",
    "    * Visualise clusters and subclusters using Kepler.gl for interactive mapping.\n",
    "    * Explore cluster characteristics using summary statistics and index scores.\n",
    "    * Export results to various formats (GeoPackage, Parquet) for use in GIS software.\n",
    "    \n",
    "* **Cluster Naming with LLMs:**\n",
    "    * Use Large Language Models (LLMs) to generate descriptive names and summaries for clusters based on their characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Geodemographics?\n",
    "\n",
    "Geodemographics is a method of classifying geographic areas based on the characteristics of their populations. It involves grouping areas with similar demographic, socio-economic, and lifestyle attributes into distinct categories or clusters. These classifications help in understanding the spatial distribution of different population segments and are widely used in various fields such as marketing, urban planning, public health, and social research.\n",
    "\n",
    "Geodemographic classifications are typically created using statistical techniques such as cluster analysis, in particular k-means clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup \n",
    "\n",
    "There are a number of python packages that need to be installed to run this notebook. It is recommended to use a virtual environment to manage these dependencies. This can be set up using the following commands in your terminal/command prompt:\n",
    "\n",
    "```bash\n",
    "# macOS / Linux\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate\n",
    "```\n",
    "or \n",
    "\n",
    "```powershell\n",
    "# Windows (PowerShell)\n",
    "python -m venv .venv\n",
    ".venv/Scripts/Activate.ps1\n",
    "```\n",
    "Once the virtual environment is activated, install the required packages using pip:\n",
    "\n",
    "```bash\n",
    "pip install --upgrade pip\n",
    "pip install pandas geopandas pyarrow scikit-learn clustergram umap-learn seaborn plotly matplotlib numpy keplergl\n",
    "```\n",
    "this can also be done using magic commands in a Jupyter notebook cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas geopandas scikit-learn matplotlib seaborn umap-learn kepler.gl plotly openai"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Import the necessary libraries and packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| warning: false\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from clustergram import Clustergram\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from keplergl import KeplerGl\n",
    "import umap.umap_ as umap\n",
    "import plotly.express as px\n",
    "from matplotlib import colors as mcolors\n",
    "import seaborn as sns\n",
    "import openai\n",
    "import json\n",
    "\n",
    "#set a  random seed for reproducibility\n",
    "random_seed = 507\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "\n",
    "# Set global font sizes (affects titles, labels, ticks, etc.)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 7,   # subplot titles\n",
    "    \"axes.labelsize\": 8,   # x/y labels\n",
    "    \"xtick.labelsize\": 6,  # x tick labels\n",
    "    \"ytick.labelsize\": 6,  # y tick labels\n",
    "    \"legend.fontsize\": 6,  # legend\n",
    "    \"figure.figsize\": (8, 6)  # default figure size\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Retrieving and Preparing Data\n",
    "We will be using census data from all four UK nations, which are available openly from the respective national statistics agencies (listed below). \n",
    "For this workshop, we will utilise a subset of census variables that have been unified across the four nations. These variables were used to produce a UK-wide Output Area Classification (OAC) in 2021 [@2021oac].\n",
    "We will also need boundary data for the Output Areas (OAs).\n",
    "All the data required can be downloaded from the Geographic Data Service Website [LINK], place the downloaded files in the `input_data` folder.\n",
    "\n",
    "Original Data Sources:\n",
    "\n",
    "1. **ONS Census 2021** (England & Wales):  \n",
    "   - [ONS Data Service](https://www.ons.gov.uk/)\n",
    "   - Downloaded from GitHub-based CSV files  \n",
    "\n",
    "2. **NRS Census 2022** (Scotland):  \n",
    "   - [Scotland's Census](https://www.scotlandscensus.gov.uk/)\n",
    "   - Downloaded from GitHub-based CSV files   \n",
    "\n",
    "3. **NISRA Census 2021** (Northern Ireland):  \n",
    "   - [NISRA Website](https://www.nisra.gov.uk/)\n",
    "   - Downloaded from GitHub-based CSV files \n",
    "   \n",
    "4. **ONS Geoportal** for boundaries and shapefiles, including clipped EW, Scotland, and Northern Ireland geographies.\n",
    "\n",
    "\n",
    "### Output Areas\n",
    "Output Areas (OAs), called data zones in northern ireland and small areas in Scotland, are the smallest geographical units available openly in the UK Census. They are designed to have similar population sizes and are used to report census data. Each OA typically contains around 100-200 households. These are the base geographical units used in this classification.\n",
    "\n",
    "![](images/OAexample.png){fig-align=\"center\" width=\"500\"}\n",
    "\n",
    "### Spatial Standardisation\n",
    "\n",
    "The data here has been normalised by the total population of each OA to give a percentage. This is important as OAs can vary in population size, and using raw counts would bias the clustering towards more populous areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the census data\n",
    "variable_df = pd.read_parquet(\"input_data/uk_census_data.parquet\")\n",
    "#set Output area lookup code as index\n",
    "variable_df = variable_df.set_index('OA')\n",
    "#round to 2 decimal places\n",
    "variable_df = variable_df.round(2)\n",
    "#look at the shape of the dataset\n",
    "print(f\"Input data shape: {variable_df.shape}\")\n",
    "#look at the first few rows of the dataset\n",
    "variable_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short descriptions of the variables used in this example are found in the file `OAC_variables.csv`. The full variable descriptions can be found in the [2021 Census User Guide](https://www.ons.gov.uk/census/censustransformationprogramme/2021census/2021censususerguide).\n",
    "\n",
    "These variables have been selected to provide a broad overview of demographic, socio-economic, and housing characteristics. They cover aspects such as age distribution, household composition, housing type, housing tenure, employment status and education levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the lookup file which contains variable descriptions.\n",
    "var_lookup = pd.read_csv(\"input_data/Variables_OAC.csv\")[[\"No.\",\"Variable_Name\",\"Domain\"]]\n",
    "var_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the data\n",
    "\n",
    "We can plot the distribution of all the variables to get a sense of their distributions. Many of the variables are highly skewed, which is common for census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas histogram plotting function with seaborn aesthetics\n",
    "sns.set_style(\"whitegrid\")\n",
    "nrows = int(np.ceil(len(variable_df.columns) / 3))\n",
    "\n",
    "variable_df_withnames = variable_df.copy()\n",
    "variable_df_withnames.columns = var_lookup['Variable_Name'].values[:58]\n",
    "variable_df_withnames.hist(bins=30, figsize=(8, nrows*1.5), edgecolor='black', layout=(nrows, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Data\n",
    "\n",
    "We will also need the Output Area boundaries to map the results. These can be downloaded from the ONS Geoportal. The file used here is a GeoPackage containing the 2021 Output Area boundaries for the whole of the UK, clipped to the extent of England and Wales, Scotland, and Northern Ireland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------\n",
    "# Import spatial data\n",
    "#---------\n",
    "\n",
    "# Output Areas (Dec 2021) Boundaries Full Clipped EW (BFC) - E&W\n",
    "# Manually downloaded file, same as in R\n",
    "OA_2021_Boundary_EW = gpd.read_file(\"input_data/boundaries/Output_Areas_2021_EW_BGC_V2_-5587136561181621407.gpkg\")\n",
    "OA_2021_Boundary_EW = OA_2021_Boundary_EW.rename(columns={\"OA21CD\": \"OA\"})[[\"OA\", \"geometry\"]]\n",
    "\n",
    "# Output Areas 2022 Scotland (GeoJSON via Google Drive link)\n",
    "OA_2022_Boundary_S = gpd.read_file(\"input_data/boundaries/Scotland_OA_22.gpkg\")\n",
    "OA_2022_Boundary_S = OA_2022_Boundary_S.rename(columns={\"code\": \"OA\"})[[\"OA\", \"geometry\"]]\n",
    "\n",
    "OA_2021_Boundary_NI = gpd.read_file(\"input_data/boundaries/DZ2021.geojson\")\n",
    "OA_2021_Boundary_NI = OA_2021_Boundary_NI.rename(columns={\"DZ2021_cd\": \"OA\"})[[\"OA\", \"geometry\"]]\n",
    "\n",
    "#---------\n",
    "# Combine OA and calculate area\n",
    "#---------\n",
    "\n",
    "# Reproject to British National Grid (EPSG:27700)\n",
    "OA_2021_Boundary_EW = OA_2021_Boundary_EW.to_crs(27700)\n",
    "OA_2022_Boundary_S = OA_2022_Boundary_S.to_crs(27700)\n",
    "OA_2021_Boundary_NI = OA_2021_Boundary_NI.to_crs(27700)\n",
    "\n",
    "\n",
    "# Merge\n",
    "OA_Boundaries = gpd.GeoDataFrame(\n",
    "    pd.concat([OA_2021_Boundary_EW, OA_2022_Boundary_S, OA_2021_Boundary_NI], ignore_index=True),\n",
    "    crs=\"EPSG:27700\"\n",
    ")\n",
    "\n",
    "# set index to OA code\n",
    "OA_Boundaries = OA_Boundaries.set_index(\"OA\")\n",
    "#---------\n",
    "# Save UK OA file\n",
    "#---------\n",
    "OA_Boundaries.to_file(\"input_data/boundaries/OA_2021_22_Boundaries.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "#---------\n",
    "# Load Local Authority District (LAD) for region selection\n",
    "#---------\n",
    "LAD_Boundaries = gpd.read_file(\"input_data/boundaries/Local_Authority_Districts_December_2022_UK_BGC_V2_5759908710055972638.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Region\n",
    "For this workshop, we will focus on a specific region of the UK to keep the analysis manageable. \n",
    "\n",
    "Focusing on a specific region allows us to create a more detailed and relevant geodemographic classification for that area, capturing local nuances and characteristics that may be lost in a broader national classification. For example, a [London specific OAC](https://data.geods.ac.uk/dataset/london-oac) was developed as London has a drastically different demographic composition to the rest of the United Kingdom [@loac]. \n",
    "\n",
    "By default we will use the Output Areas within the Liverpool City Region covering the city of liverpool and its surrounding areas. This region is prodominently urban and has a diverse population, making it an interesting case study for geodemographic classification.\n",
    "If running this notebook on your own machine, you can change the region of study from the selection below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region definitions (LAD22CD codes)\n",
    "region_lads = {\n",
    "    \"Greater Manchester\": [\n",
    "        \"E08000001\",\"E08000002\",\"E08000003\",\"E08000004\",\"E08000005\",\n",
    "        \"E08000006\",\"E08000007\",\"E08000008\",\"E08000009\",\"E08000010\"\n",
    "    ],\n",
    "    \"Liverpool City Region\": [\n",
    "        \"E06000006\",\"E08000011\",\"E08000012\",\"E08000013\",\"E08000014\",\"E08000015\"\n",
    "    ],\n",
    "    \"Greater London\": [\n",
    "        \"E09000001\",\"E09000002\",\"E09000003\",\"E09000004\",\"E09000005\",\n",
    "        \"E09000006\",\"E09000007\",\"E09000008\",\"E09000009\",\"E09000010\",\n",
    "        \"E09000011\",\"E09000012\",\"E09000013\",\"E09000014\",\"E09000015\",\n",
    "        \"E09000016\",\"E09000017\",\"E09000018\",\"E09000019\",\"E09000020\",\n",
    "        \"E09000021\",\"E09000022\",\"E09000023\",\"E09000024\",\"E09000025\",\n",
    "        \"E09000026\",\"E09000027\",\"E09000028\",\"E09000029\",\"E09000030\",\n",
    "        \"E09000031\",\"E09000032\",\"E09000033\"\n",
    "    ],\n",
    "    \"Scotland\": [\n",
    "        \"S12000005\",\"S12000006\",\"S12000008\",\"S12000010\",\"S12000011\",\n",
    "        \"S12000013\",\"S12000014\",\"S12000017\",\"S12000018\",\"S12000019\",\n",
    "        \"S12000020\",\"S12000021\",\"S12000023\",\"S12000026\",\"S12000027\",\n",
    "        \"S12000028\",\"S12000029\",\"S12000030\",\"S12000033\",\"S12000034\",\n",
    "        \"S12000035\",\"S12000036\",\"S12000038\",\"S12000039\",\"S12000040\",\n",
    "        \"S12000041\",\"S12000042\",\"S12000045\",\"S12000047\",\"S12000048\",\n",
    "        \"S12000049\",\"S12000050\"\n",
    "    ],\n",
    "    \"Northern Ireland\": [\n",
    "        \"N09000001\",\"N09000002\",\"N09000003\",\"N09000004\",\"N09000005\",\n",
    "        \"N09000006\",\"N09000007\",\"N09000008\",\"N09000009\",\"N09000010\",\n",
    "        \"N09000011\"\n",
    "    ],\n",
    "    \"Wales\": [\n",
    "        \"W06000001\",\"W06000002\",\"W06000003\",\"W06000004\",\"W06000005\",\n",
    "        \"W06000006\",\"W06000007\",\"W06000008\",\"W06000009\",\"W06000010\",\n",
    "        \"W06000011\",\"W06000012\",\"W06000013\",\"W06000014\",\"W06000015\",\n",
    "        \"W06000016\",\"W06000017\",\"W06000018\",\"W06000019\",\"W06000020\",\n",
    "        \"W06000021\",\"W06000022\",\"W06000023\",\"W06000024\"\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the region of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose region from the list above by uncommenting the relevant line\n",
    "lad_codes = region_lads[\"Liverpool City Region\"]\n",
    "# #or eg:\n",
    "# lad_codes = region_lads[\"Northern Ireland\"]\n",
    "\n",
    "# Spatial join to filter only intersecting OAs\n",
    "oas_region = gpd.sjoin(OA_Boundaries, LAD_Boundaries[LAD_Boundaries[\"LAD22CD\"].isin(lad_codes)], predicate=\"intersects\")\n",
    "\n",
    "# --- Merge OA polygons with your variable data keeping only those with matching OAs in the region\n",
    "oas_region_vars = oas_region.join(variable_df, how=\"inner\")\n",
    "\n",
    "# #keeo only OAs in our region\n",
    "variable_df_region=variable_df.loc[variable_df.index.isin(oas_region.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "region_geom = unary_union(\n",
    "    LAD_Boundaries.loc[LAD_Boundaries[\"LAD22CD\"].isin(lad_codes), \"geometry\"]\n",
    ")\n",
    "region_centroid = region_geom.centroid\n",
    "centroid_ll = gpd.GeoSeries([region_centroid], crs=LAD_Boundaries.crs).to_crs(epsg=4326).iloc[0]\n",
    "centroid_lat, centroid_lon = centroid_ll.y, centroid_ll.x\n",
    "\n",
    "# --- Quick Kepler map ---\n",
    "area_map_ = KeplerGl(\n",
    "    height=600,\n",
    "    config={\n",
    "        \"version\": \"v1\",\n",
    "        \"config\": {\n",
    "            \"mapState\": {\n",
    "               'latitude': centroid_lat,\n",
    "                'longitude': centroid_lon,\n",
    "                \"zoom\": 9,\n",
    "                \"pitch\": 0,\n",
    "                \"bearing\": 0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "# Add your layer\n",
    "area_map_.add_data(data=oas_region_vars)\n",
    "# If you want to export to a standalone HTML:\n",
    "area_map_.save_to_html(file_name=\"outputs/maps/region_oas_map.html\")\n",
    "# Show inside Jupyter\n",
    "area_map_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"outputs/maps/region_oas_map.html\" width=\"100%\" height=\"650\" style=\"border:none;\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform and Standardize Variables\n",
    "\n",
    "Before applying clustering algorithms, it is important to transform and standardize the data to ensure that all variables contribute equally to the analysis.\n",
    "\n",
    "The function below applies two transformations to a dataframe (applied column-wise):\n",
    "\n",
    "1. **Inverse Hyperbolic Sine (IHS) transform**  \n",
    "2. **Min–Max scaling to [0, 1]**\n",
    "\n",
    "### Mathematical definitions\n",
    "\n",
    "**Inverse hyperbolic sine (IHS, a.k.a. arcsinh):**  \n",
    "- Similar to a log transform but works with zero and negative values.  \n",
    "- Helps stabilize variance and make skewed distributions more normal-like.  \n",
    "\n",
    "$$\n",
    "\\mathrm{arcsinh}(x) = \\ln\\!\\big(x + \\sqrt{x^{2}+1}\\big)\n",
    "$$\n",
    "\n",
    "**Properties:**\n",
    "- For large \\(|x|\\):  \n",
    "  $$\n",
    "  \\mathrm{arcsinh}(x) \\approx \\ln(2|x|)\n",
    "  $$\n",
    "  (behaves like log).  \n",
    "\n",
    "- Near \\(0\\):  \n",
    "  $$\n",
    "  \\mathrm{arcsinh}(x) \\approx x\n",
    "  $$\n",
    "\n",
    " \n",
    "\n",
    "**Min–Max scaling (applied per column after IHS):**\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
    "$$\n",
    "\n",
    "- Rescales all values into the range \\([0, 1]\\).  \n",
    "- Useful for comparing variables with different units/scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_standardize_data(df):\n",
    "    \"\"\"Apply inverse hyperbolic sine transform, to account for non-normality\n",
    "    and then range standardize using min-max scaling to the dataframe.\"\"\"\n",
    "    df = np.arcsinh(df)\n",
    "    denom = df.max() - df.min()\n",
    "    df = (df - df.min()) / denom.replace(0, 1)  # prevent divide-by-zero\n",
    "    return df\n",
    "\n",
    "# Transform the input data\n",
    "transformed_variable_df = transform_and_standardize_data(variable_df_region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Selection\n",
    "In geodemographics, **variable selection** is crucial to turn large datasets (like the Census) with 100s of variables into meaningful clusters.  \n",
    "The nature of clustering means the high-dimensional space can be sparse and noisy, so reducing the number of variables helps improve cluster quality and interpretability.\n",
    "\n",
    "Key points:  \n",
    "- **Intention** – variables depend on the purpose of the geodem (e.g. retail vs. health).  \n",
    "- **Correlation** – drop highly correlated variables to avoid redundancy.  \n",
    "- **Variance** – keep variables that vary across places (so they can distinguish areas).  \n",
    "- **Expert choice** – ensure selected variables are socially meaningful.  \n",
    "\n",
    "Here we are using a **broad, pre-selected dataset** which was used for the UK OAC 2021 classification. So we expect the variables to be broadly suitable for our region of interest.\n",
    "\n",
    "New methods include:  \n",
    "- **Automated variable selection** [@pcavarselect] – uses statistical procedure to determine a subset of variables which produce the best clustering results.  \n",
    "- **Autoencoders** (see Alex’s talk tomorrow!) – Use neural networks to compress all variables into a smaller set of features, while preserving the most important patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| out.width: \"100%\"\n",
    "\n",
    "def check_corrs(df, corr_threshold=0.9):\n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    # --- Interactive heatmap with Plotly ---\n",
    "    fig = px.imshow(\n",
    "        corr_matrix.values,\n",
    "        color_continuous_scale=\"RdBu_r\",\n",
    "        zmin=-1, zmax=1,\n",
    "        title=\"Correlation Heatmap\",\n",
    "        x=corr_matrix.columns,\n",
    "        y=corr_matrix.columns,\n",
    "    )\n",
    "    # Add hover names for tooltips\n",
    "    fig.update_traces(\n",
    "        hovertemplate=\"<b>%{x}</b> vs <b>%{y}</b><br>Correlation: %{z:.3f}<extra></extra>\",\n",
    "    )\n",
    "    # Hide x/y tick labels but keep tooltips\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "\n",
    "    # fig.update_layout(width=800, height=800)\n",
    "    fig.show()\n",
    "\n",
    "    # --- Find highly correlated pairs (print each pair twice) ---\n",
    "    cols = corr_matrix.columns\n",
    "    # consider only upper triangle to get unique unordered pairs (i < j)\n",
    "    upper_mask = np.triu(np.ones(corr_matrix.shape, dtype=bool), k=1)\n",
    "    # build mask for correlations above threshold (and exclude perfect 1.0)\n",
    "    corr_vals = corr_matrix.values\n",
    "    mask = (corr_vals > corr_threshold) & (corr_vals < 1) & upper_mask\n",
    "\n",
    "    for i, j in zip(*np.where(mask)):\n",
    "        col_i, col_j = cols[i], cols[j]\n",
    "        val = corr_matrix.iat[i, j]\n",
    "        # print the pair twice (both directions)\n",
    "        print(f\"High correlation between {col_i} and {col_j}: {val:.3f}\")\n",
    "        print(f\"High correlation between {col_j} and {col_i}: {val:.3f}\")\n",
    "\n",
    "\n",
    "transformed_variable_df_withnames = transformed_variable_df.copy()\n",
    "transformed_variable_df_withnames.columns = [var_lookup.set_index(\"No.\")[\"Variable_Name\"].to_dict().get(col, col) for col in transformed_variable_df.columns]\n",
    "\n",
    "# Check for highly correlated variables and remove them, set the thereshold as desired.\n",
    "check_corrs(\n",
    "    transformed_variable_df_withnames, corr_threshold=0.90)\n",
    "\n",
    "#varience check\n",
    "# Calculate variance for each column\n",
    "variances = transformed_variable_df_withnames.var()\n",
    "#plot the variances\n",
    "plt.figure(figsize=(8, 5))\n",
    "variances.sort_values().plot(kind='bar')\n",
    "plt.ylabel('Variance')\n",
    "plt.title('Variance of Each Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Variables\n",
    "If we want to remove any variables we can do so here. \n",
    "This could be based on the analysis above or to tailor the classification to a specific purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_vars = ['v02', 'v04']  # Replace with actual variable names to drop\n",
    "drop_vars = [] # Example: No variables to drop\n",
    "# for northern ireland #bangladeshi ethnicity now removed as no variation as no bangladeshi in northern ireland\n",
    "# drop_vars = ['v12'] \n",
    "\n",
    "cleaned_variable_df = transformed_variable_df.drop(columns=drop_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "K-means clustering is the most commonly used clustering algorithm for geodemographic classification.\n",
    "\n",
    "\n",
    "K-Means is one of the most widely used clustering algorithms in unsupervised machine learning. It partitions a dataset into *k* groups (clusters), where each observation belongs to the cluster with the nearest mean (centroid). The algorithm iteratively updates cluster assignments and centroids until convergence.\n",
    "\n",
    "**How it works (simplified):**\n",
    "\n",
    "1. Choose the number of clusters (*k*).  \n",
    "2. Initialize *k* centroids (randomly or using methods like *k-means++*).  \n",
    "3. Assign each data point to the nearest centroid.  \n",
    "4. Update centroids as the mean of the points in each cluster.  \n",
    "5. Repeat steps 3–4 until assignments no longer change (or improvement is below a threshold).\n",
    "\n",
    "**Strengths:**\n",
    "- Simple and computationally efficient.  \n",
    "- Works well when clusters are roughly spherical and similar in size.  \n",
    "\n",
    "**Limitations:**\n",
    "- Requires specifying *k* in advance.  \n",
    "- Sensitive to outliers and scaling of features.  \n",
    "- Assumes clusters are convex and isotropic, which may not hold in real data.  \n",
    "\n",
    "More details and examples can be found here: [Scikit-learn: K-Means](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "\n",
    "\n",
    "## Choosing the number of clusters (k) - Clustergrams\n",
    "\n",
    "When using k-means clustering, one of the key decisions is selecting the optimal number of clusters (k). This choice can significantly impact the quality and interpretability of the resulting geodemographic classification.\n",
    "Key considerations when choosing k:\n",
    "- Each cluster be as homogeneous as possible.\n",
    "- Each cluster should be as distinct from the others as possible.\n",
    "- The clusters should be as evenly sized as possible.\n",
    "\n",
    "Clustergrams are visualisation technique that shows how cluster assignments change as you increase the number of clusters (k). This helps you to understand the structure in very high-dimensional space in the following ways:\n",
    "\n",
    "- **Cluster separation**: Helps you to determine the right number of clusters by visualising how cleanly clusters separate  \n",
    "- **Cluster stability**: Shows which clusters persist across different k values (stable long lines) vs. those which are artifacts of over-clustering (short, erratic lines)  \n",
    "- **Split patterns**: Reveals the natural hierarchy in the data by showing how clusters subdivide  \n",
    "\n",
    "\n",
    "Further guidance on interpreting clustergrams and choosing the number of clusters can be found here:  [Clustergram](https://clustergram.readthedocs.io/en/stable/notebooks/introduction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clustergram(df, n_init, save_loc, random_seed=random_seed):\n",
    "    \"\"\"\n",
    "    Create and save a clustergram for evaluating k-means clustering solutions.\n",
    "\n",
    "    The clustergram visualizes clustering stability and helps identify the optimal \n",
    "    number of clusters by performed the k-means algorithm for a range of cluster\n",
    "    numbers.\n",
    "    Since k-means is sensitive to initialization, `n_init` determines the number of \n",
    "    times the algorithm runs with different centroid seeds. The final result is the \n",
    "    best outcome based on inertia/WCSS (within-cluster sum of squares.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame or np.ndarray): The input data for clustering.\n",
    "    n_init (int): Number of k-means runs with different initial centroid seeds. \n",
    "                  Higher values (e.g., ~1000) improve solution stability but increase runtime.\n",
    "    save_loc (str): File path to save the clustergram plot.\n",
    "    random_seed (int, optional): Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    cgram = Clustergram(range(1, 10), n_init=n_init, random_state=random_seed,verbose=False)  # Initialize clustergram model\n",
    "    cgram.fit(df)  # Fit model to data\n",
    "    cgram.plot()  # Generate plot\n",
    "    plt.savefig(save_loc)  # Save figure\n",
    "    plt.show()  # Display plot\n",
    "\n",
    "# Example usage\n",
    "n_init = 100  # Use a low value for quick testing, increase (~100) for final results\n",
    "create_clustergram(cleaned_variable_df, n_init, save_loc=\"outputs/plots/supergroup_clustergram.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the number of clusters (k) based on the clustergram above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of clusters (K). Choose K based on the clustergram plot.\n",
    "num_clusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans(input_df, num_clusters, n_init, output_filepath, random_seed=None):\n",
    "    \"\"\"\n",
    "    Run K-means clustering on the input dataset and save the cluster assignments.\n",
    "\n",
    "    This function applies K-means clustering to the provided dataset, assigns cluster \n",
    "    labels to each row, and saves the cluster assignments as a lookup table.\n",
    "\n",
    "    Parameters:\n",
    "    input_df (pd.DataFrame): The input dataset to be clustered.\n",
    "    num_clusters (int): The number of clusters (K) to create.\n",
    "    n_init (int): Number of times the K-means algorithm runs with different initial \n",
    "                  centroid seeds. The best result based on inertia/WCSS is chosen. \n",
    "                  A higher value (e.g., ~1000) is recommended for final results, \n",
    "                  but a lower value can be used for testing.\n",
    "    output_filepath (str): Path to save the resulting cluster assignments.\n",
    "    random_seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The input DataFrame with an added 'cluster' column containing \n",
    "                  the assigned cluster for each row.\n",
    "    \"\"\"\n",
    "    df = input_df.copy()\n",
    "    # Initialize the K-means model\n",
    "    kmeans_model = KMeans(n_clusters=num_clusters,init=\"random\", random_state=random_seed, n_init=n_init)\n",
    "    \n",
    "    # Fit the model and assign clusters\n",
    "    df['cluster'] = kmeans_model.fit_predict(df)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "\n",
    "    # Save the cluster assignments to a CSV file\n",
    "    df[['cluster']].to_csv(output_filepath)\n",
    "    return df\n",
    "\n",
    "\n",
    "n_init = 100 # Use a low value for quick testing, increase (~100) for final results\n",
    "output_filepath = \"outputs/supergroups_clusteroutput.csv\"\n",
    "print(f\"Output file path: {output_filepath}\")\n",
    "\n",
    "# Run K-means clustering\n",
    "supergrouped_variable_df = run_kmeans(cleaned_variable_df, num_clusters, n_init=n_init, output_filepath = output_filepath, random_seed=random_seed)\n",
    "\n",
    "# Map numeric labels to letters\n",
    "label_map = {i: chr(65 + i) for i in range(num_clusters)}  # 0->A, 1->B, etc.\n",
    "supergrouped_variable_df['cluster'] = supergrouped_variable_df['cluster'].map(label_map)\n",
    "\n",
    "#supergrouped_variable_df contains the cluster assignments for each row in the input data, and the input data itself.\n",
    "supergrouped_variable_df[\"cluster\"].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the clusters on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "#| echo: false\n",
    "\n",
    "\n",
    "# Custom cluster colours for up to 10 clusters\n",
    "colours = {\n",
    "    \"A\": '#8dd3c7',\n",
    "    \"B\": '#ffffb3',\n",
    "    \"C\": '#bebada',\n",
    "    \"D\": '#fb8072',\n",
    "    \"E\": '#fdb462',\n",
    "    \"F\": \"#235477\",\n",
    "    \"G\": '#fccde5',\n",
    "    \"H\": '#d9d9d9',\n",
    "    \"I\": '#bc80bd',\n",
    "    \"J\": '#ccebc5'\n",
    "}\n",
    "\n",
    "sorted_clusters = sorted(colours.keys())\n",
    "colour_list = [colours[k] for k in sorted_clusters]\n",
    "\n",
    "# #append geometry column from oas_region to supergrouped_variable_df to make a geodataframe\n",
    "gdf = oas_region.merge(supergrouped_variable_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Config for map\n",
    "config = {\n",
    "    \"version\": \"v1\",\n",
    "    \"config\": {\n",
    "        \"visState\": {\n",
    "            \"filters\": [],\n",
    "            \"layers\": [\n",
    "                {\n",
    "                    \"id\": \"clusters_layer\",\n",
    "                    \"type\": \"geojson\",   # polygon layer\n",
    "                    \"config\": {\n",
    "                        \"dataId\": \"clusters\",\n",
    "                        \"label\": \"Clusters\",\n",
    "                        'columns': {'geojson': 'geometry'},\n",
    "                        \"color\": [130, 154, 227],\n",
    "                        \"highlightColor\": [252, 242, 26, 255],\n",
    "                        \"isVisible\": True,\n",
    "                        \"visConfig\": {\n",
    "                            \"opacity\": 0.8,\n",
    "                            \"thickness\": 0,\n",
    "                            \"strokeColor\": None,\n",
    "                            \"colorRange\": {\n",
    "                                \"name\": \"Custom\",\n",
    "                                \"type\": \"qualitative\",\n",
    "                                \"category\": \"Custom\",\n",
    "                                \"colors\": colour_list\n",
    "                            },\n",
    "                            \"filled\": True\n",
    "                        },\n",
    "                        \"hidden\": False,\n",
    "                        \"textLabel\": []\n",
    "                    },\n",
    "                    \"visualChannels\": {\n",
    "                        \"colorField\": {\"name\": \"cluster\", \"type\": \"string\"},\n",
    "                        \"colorScale\": \"ordinal\",\n",
    "                        \"strokeColorField\": None,\n",
    "                        \"strokeColorScale\": \"quantile\",\n",
    "                        \"sizeField\": None,\n",
    "                        \"sizeScale\": \"linear\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"effects\": [],\n",
    "            \"interactionConfig\": {\n",
    "                \"tooltip\": {\n",
    "                    \"fieldsToShow\": {\n",
    "                        \"clusters\": [\n",
    "                            {\"name\": \"cluster\", \"format\": None}\n",
    "                        ]\n",
    "                    },\n",
    "                    \"enabled\": True\n",
    "                },\n",
    "                \"legend\": {\n",
    "                    \"enabled\": True,\n",
    "                    \"active\": True\n",
    "                }\n",
    "            },\n",
    "\n",
    "            \"layerBlending\": \"normal\"\n",
    "        },\n",
    "        \"mapState\": {\n",
    "            \"bearing\": 0,\n",
    "            \"dragRotate\": False,\n",
    "               'latitude': centroid_lat,\n",
    "            'longitude': centroid_lon,\n",
    "            \"pitch\": 0,\n",
    "            \"zoom\": 9,\n",
    "            \"isSplit\": False\n",
    "        },\n",
    "        \"mapStyle\": {\n",
    "            \"styleType\": \"dark-matter\",\n",
    "            \"topLayerGroups\": {\n",
    "                \"water\": True,\n",
    "                \"building\": True\n",
    "            },\n",
    "            \"visibleLayerGroups\": {\n",
    "                \"label\": True,\n",
    "                \"road\": True,\n",
    "                \"border\": False,\n",
    "                \"building\": True,\n",
    "                \"water\": True,\n",
    "                \"land\": True,\n",
    "                \"3d building\": False\n",
    "            },\n",
    "            \"mapStyles\": {\n",
    "                \"dark-matter\": {\n",
    "                    \"id\": \"dark-matter\",\n",
    "                    \"label\": \"Carto Dark Matter\",\n",
    "                    \"url\": \"https://basemaps.cartocdn.com/gl/dark-matter-gl-style/style.json\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Show in Kepler\n",
    "map_with_basemap = KeplerGl(data={\"clusters\":gdf}, config=config, height=600)\n",
    "map_with_basemap.save_to_html(file_name=\"outputs/maps/cluster_map.html\")\n",
    "map_with_basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"outputs/maps/cluster_map.html\" width=\"100%\" height=\"650\" style=\"border:none;\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP Visualization\n",
    "\n",
    "We can use UMAP (Uniform Manifold Approximation and Projection) to visualise the high-dimensional census data in 2D. UMAP is a dimensionality reduction technique that preserves both local and global structure in the data, making it well-suited for visualising complex datasets like census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features = all columns except the cluster label\n",
    "features = [c for c in supergrouped_variable_df.columns if c != 'cluster']\n",
    "# Extract features and labels  (transformed)\n",
    "X = supergrouped_variable_df[features].values\n",
    "labels = supergrouped_variable_df['cluster'].values\n",
    "\n",
    "# Fit UMAP\n",
    "# Apply UMAP to reduce 64 dimensions to 2D\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=30,        # Numbers of neighbours\n",
    "    min_dist=0.0,          # Allow points to be closer together\n",
    "    n_components=2,        # Reduce to 2D for visualsation\n",
    "    random_state=random_seed,       # For reproducible results\n",
    "    metric='cosine',       # Cosine similarity works well for embeddings\n",
    "    init='random',         # Use random initialisation\n",
    "    n_epochs=500,          # More epochs for better convergence\n",
    "    spread=1.0,            # Controls how tightly points are packed\n",
    "    verbose=False          # Show progress\n",
    ")\n",
    "\n",
    "embedding = reducer.fit_transform(X)\n",
    "\n",
    "umap_results = pd.DataFrame({\n",
    "    'UMAP1': embedding[:, 0],\n",
    "    'UMAP2': embedding[:, 1],\n",
    "    'Cluster': labels\n",
    "})\n",
    "\n",
    "# Save the UMAP results\n",
    "umap_results.to_parquet('outputs/umap_results.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| out.width: \"100%\"\n",
    "\n",
    "\n",
    "# Define colours for each cluster - same as earlier map\n",
    "colours = {\n",
    "    \"A\": '#8dd3c7',\n",
    "    \"B\": '#ffffb3',\n",
    "    \"C\": '#bebada',\n",
    "    \"D\": '#fb8072',\n",
    "    \"E\": '#fdb462',\n",
    "    \"F\": \"#235477\",\n",
    "    \"G\": '#fccde5',\n",
    "    \"H\": '#d9d9d9',\n",
    "    \"I\": '#bc80bd',\n",
    "    \"J\": '#ccebc5'\n",
    "}\n",
    "\n",
    "# Create interactive UMAP scatter plot\n",
    "fig_interactive = px.scatter(\n",
    "    umap_results,\n",
    "    x='UMAP1',\n",
    "    y='UMAP2',\n",
    "    color='Cluster',\n",
    "    category_orders={\"Cluster\": sorted(umap_results[\"Cluster\"].unique())},  #\n",
    "    color_discrete_map=colours,  \n",
    ")\n",
    "\n",
    "# Style tweaks\n",
    "fig_interactive.update_traces(marker=dict(size=3, opacity=0.7))\n",
    "fig_interactive.update_layout(\n",
    "    title=\"UMAP Projection of Clusters\",\n",
    "    xaxis_title=\"UMAP 1\",\n",
    "    yaxis_title=\"UMAP 2\",\n",
    "    legend_title=\"Cluster\"\n",
    ")\n",
    "\n",
    "\n",
    "#save to html\n",
    "fig_interactive.write_html(\"outputs/umap_interactive.html\")\n",
    "fig_interactive.update_layout(width=800, height=600)\n",
    "fig_interactive.show(config={\"responsive\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UMAP projection indicates further structure within clusters\n",
    "More on that in a bit.. for now lets dig into the clusters that we've got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Profiling, Naming, and Describing using Language Models\n",
    "\n",
    "We can explore the characteristics of each cluster using summary statistics and index scores. This helps us understand each cluster.\n",
    "\n",
    "## Cluster Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lot at the characteristics of each cluster\n",
    "\n",
    "# Read in the data\n",
    "pop_size = pd.read_csv(\"input_data/oa_pop_data.csv\")\n",
    "pop_size = pop_size.set_index('OA')\n",
    "#rename column to \"population\"\n",
    "pop_size = pop_size.rename(columns={'uk001001': 'population'})\n",
    "pop_size = pop_size['population']\n",
    "\n",
    "#basic statistics of each cluster, number (perc of OAs) in each cluster and population\n",
    "\n",
    "#number of OAs in each cluster\n",
    "cluster_counts = supergrouped_variable_df['cluster'].value_counts().sort_index()\n",
    "#percentage of OAs in each cluster\n",
    "cluster_perc = (cluster_counts / cluster_counts.sum() * 100).round(2)\n",
    "\n",
    "#join pop_size to supergrouped_variable_df on index\n",
    "supergrouped_variable_df_withpop = supergrouped_variable_df.join(pop_size, how='left')\n",
    "\n",
    "#pop in each cluster\n",
    "cluster_pop = supergrouped_variable_df_withpop.groupby('cluster')['population'].sum()\n",
    "#percentage of pop in each cluster\n",
    "cluster_pop_perc = (cluster_pop / cluster_pop.sum() * 100).round(2)\n",
    "\n",
    "#combine into a dataframe\n",
    "cluster_summary = pd.DataFrame({\n",
    "    'Number of OAs': cluster_counts,\n",
    "    'Percentage of OAs': cluster_perc,\n",
    "    'Population': cluster_pop,\n",
    "    'Percentage of Population': cluster_pop_perc\n",
    "})\n",
    "cluster_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Profiling\n",
    "\n",
    "Index scores are a way to summarise how a particular variable behaves within a cluster compared to the overall average. They help identify which characteristics are over- or under-represented in each cluster.\n",
    "\n",
    "Index scores are calculated as follows:\n",
    "\n",
    "$$\n",
    "\\text{Index Score} = \\left( \\frac{\\text{Mean of Variable in Cluster}}{\\text{Overall Mean of Variable}} \\right) \\times 100\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- **Mean of Variable in Cluster**: the average value of the variable for all areas within the specific cluster.  \n",
    "- **Overall Mean of Variable**: the average value of the variable across all areas in the dataset.  \n",
    "\n",
    "Here we will look only at variables used in the clustering. It can also be useful to look at variables not used in the clustering or from other data sources to help understand the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map encoding -> human name\n",
    "encoding_to_name = dict(zip(var_lookup[\"No.\"], var_lookup[\"Variable_Name\"]))\n",
    "\n",
    "features = [c for c in supergrouped_variable_df.columns if c != 'cluster']\n",
    "#dont average the cluster column\n",
    "cluster_means = supergrouped_variable_df.groupby('cluster').mean()\n",
    "global_means = supergrouped_variable_df[features].mean()\n",
    "global_stds = supergrouped_variable_df[features].std()\n",
    "\n",
    "# --- Calculate percentage difference ---\n",
    "pct_diff = (cluster_means / global_means) * 100\n",
    "#drop columns with nan\n",
    "pct_diff = pct_diff.dropna(axis=1, how='any')\n",
    "pct_display_df = pct_diff.T  # index: encodings\n",
    "\n",
    "\n",
    "# --- Calculate percentage difference ---\n",
    "pct_diff = (cluster_means / global_means) * 100\n",
    "#drop columns with nan\n",
    "pct_diff = pct_diff.dropna(axis=1, how='any')\n",
    "pct_display_df = pct_diff.T  # index: encodings\n",
    "\n",
    "# build customdata for hover (human names repeated across clusters)\n",
    "human_names = pct_display_df.index.map(lambda e: encoding_to_name.get(e, e)).values\n",
    "customdata_pct = np.tile(human_names.reshape(-1, 1), (1, pct_display_df.shape[1]))\n",
    "\n",
    "# get symmetric range around 100\n",
    "max_abs = np.nanmax(np.abs(pct_display_df.values - 100))\n",
    "\n",
    "# --- Heatmap (percentage difference) ---d\n",
    "fig_pct = px.imshow(\n",
    "    pct_display_df,\n",
    "    color_continuous_scale=\"RdYlGn\",\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    labels=dict(x=\"Cluster\", y=\"Feature (encoding)\", color=\"% of global mean\"),\n",
    "    zmin=0,\n",
    "    zmax=200\n",
    ")\n",
    "\n",
    "# attach customdata and set hover\n",
    "fig_pct.data[0].customdata = customdata_pct\n",
    "fig_pct.update_traces(\n",
    "    hovertemplate=\"Cluster: %{x}<br>Encoding: %{y}<br>Name: %{customdata}<br>% of Global Mean: %{z:.1f}%<extra></extra>\",\n",
    "    zmid=100  # centre colours on 100%\n",
    ")\n",
    "\n",
    "fig_pct.update_layout(\n",
    "    title=\"Cluster Profiles (% of Global Mean)\",\n",
    "    xaxis_title=\"Cluster\",\n",
    "    yaxis_title=\"Feature (encoding)\",\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig_pct.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Cluster Naming (and description)\n",
    "\n",
    "To create a useful geodemographic classification, we need to assign meaningful names and descriptions to each cluster. This helps in interpreting the clusters and communicating their characteristics effectively. \n",
    "Traditionally, this is done manually by examining the statistical profiles of each cluster (often with input (as we produced) sometimes with external ) and using domain knowledge to assign names. Either done by a single expert, panel or utilising crowd sourcing approaches. Either way it is a time consuming process. However, we can leverage Large Language Models (LLMs) to assist in this process. We have demonstrated that LLMs can be used to generate initial name and description suggestions based on the statistical profiles of each cluster [@llmpaper].\n",
    "We can use LLMs to generate initial name and description suggestions based on the statistical profiles of each cluster.\n",
    "Again here we are using only the variables used in the clustering. It can also be useful to include other variables or external data to provide more context for the LLM.\n",
    "\n",
    "### Using the OpenAI API\n",
    "Below I use the OpenAI API, if you have an API key insert it in an .env file as;\n",
    "OPENAI_API_KEY=\"sk....sA\"\n",
    "\n",
    "If you do have an API key skip the next cell and go to the cell where we will use a browser prompt to get the cluster names and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "#| echo: true\n",
    "#| output: true\n",
    "\n",
    "#get your OpenAI API key from environment variable\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"Please set the OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "client = openai.OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# -------------------------\n",
    "# JSON Schema for output\n",
    "# -------------------------\n",
    "cluster_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"description\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"name\", \"description\"],\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"You are a geodemographic analyst. \n",
    "Your task is to produce commercial-style geodemographic cluster pen portraits \n",
    "and cluster names.\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "A geodemographics company is trying to explain the characteristics of several \n",
    "neighborhoods to a new customer. They present data comparing each \n",
    "neighborhood to the region average. A score of 100 means the neighborhood \n",
    "is equivalent to the regional average, 150 means one and a half times, 200 means twice, 50 means half, \n",
    "and 300 means three times the regional average. \n",
    "\n",
    "The description of each neighborhood should focus on characteristics with scores above 120 or below 80. \n",
    "Write in the third person, no more than 500 words. \n",
    "Do not mention the specific scores. Instead, describe patterns relative to the regional average (above/below). \n",
    "\n",
    "In the style of a commercial geodemographic classification, create a cluster name that summarises the pen portrait. \n",
    "The name should capture as many different characteristics as possible and be no more than 3 words.\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------\n",
    "# Loop through clusters\n",
    "# -------------------------\n",
    "cluster_summaries = {}\n",
    "for cluster in pct_diff.index:\n",
    "    cluster_pct = pct_diff.loc[cluster]\n",
    "\n",
    "    cluster_data = {\n",
    "        \"cluster\": cluster,\n",
    "        \"data\": {\n",
    "            encoding_to_name.get(feature, feature): round(value, 1)\n",
    "            for feature, value in cluster_pct.items()\n",
    "        },\n",
    "    }\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
    "            {\"role\": \"user\", \"content\": user_prompt.strip()},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(cluster_data)},\n",
    "        ],\n",
    "        text={\n",
    "            \"format\": {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"name\": \"cluster_summary\",\n",
    "                \"schema\": cluster_schema,\n",
    "                \"strict\": True,\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    cluster_summary = json.loads(response.output_text)\n",
    "\n",
    "    # store under your cluster ID\n",
    "    cluster_summaries[cluster] = cluster_summary\n",
    "\n",
    "cluster_summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt for browser based LLM\n",
    "The follow code cell generates a prompt for to be used in an LLM of your choice.\n",
    "Try it in your browser based LLM of choice (e.g. chatGPT, Claude, Gemini, etc)\n",
    "\n",
    "The prompt should insure that the LLM produses the output in the correct format but this cannot beguaranteed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the prompt to copy and paste into a LLM to generate cluster descriptions\n",
    "\n",
    "prompt_intial = \"\"\"\n",
    "A geodemographics company is trying to explain the characteristics of several neighborhoods to a new customer. \n",
    "They present data comparing each neighborhood to the region average. \n",
    "A score of 100 means the neighborhood is equivalent to the national average, \n",
    "a score of 150 means the neighborhood is one and a half times the national average, \n",
    "a score of 200 means the neighborhood is twice the national average, \n",
    "a score of 50 means the neighborhood is half of the region average, \n",
    "a score of 300 means the neighborhood is three times the region average. \n",
    "\n",
    "Each neighborhood has the following characteristics, described in #DATA# below. \n",
    "Data are presented for each characteristic followed by a colon, and then a score. \n",
    "The description of each neighborhood should focus on characteristics that have scores which are greater than 120 or less than 80.\n",
    "Write a separate description for each cluster (Cluster A, Cluster B, Cluster C, Cluster D, etc. \n",
    "Each description should be written in the third person, in no more than 500 words. Do not mention the specific scores from the #DATA#. \n",
    "Instead, use descriptive words to illustrate rates that are above or below the regional average.\n",
    "Make comparisons to the regional average, do not talk in absolute terms.\n",
    "\"\"\"\n",
    "\n",
    "prompt_data =\"\"\n",
    "# print the index scores for each cluster in this format:\n",
    "\n",
    "for cluster in pct_diff.index:\n",
    "    prompt_data += f\"\\n#DATA# cluster_key: {cluster}\\n\"\n",
    "    cluster_pct = pct_diff.loc[cluster]\n",
    "    for feature, value in cluster_pct.items():\n",
    "        feature_name = encoding_to_name.get(feature, feature)\n",
    "        prompt_data += f\"{feature_name}: {value:.1f}\\n\"\n",
    "\n",
    "prompt_struc = \"\"\"\n",
    "In the style of a commercial geodemographic classification; create a cluster name \n",
    "that would summarise the created geodemographic pen portraits. The names should capture as many \n",
    "different characteristics contained within the description as possible. \n",
    "The cluster name should be no more than 3 words.\n",
    "Return your response in JSON format with the structure: {\"cluster_key_1\": {\"name\": \"\", \"description\": \"\"},\"cluster_key_2\": {\"name\": \"\", \"description\": \"\"},...}\"\"\"\n",
    "\n",
    "full_prompt = prompt_intial + prompt_data + prompt_struc\n",
    "print(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy result in here;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_summaries = {\n",
    "  \"A\": {\n",
    "    \"name\": \"Elderly Communal Flats\",\n",
    "    \"description\": \"Cluster A neighborhoods feature a notably higher proportion of very elderly residents compared to the regional average. They also have significantly more individuals living in communal establishments than typical in the region. Housing in these areas shows a markedly elevated presence of flats, maisonettes, or apartments relative to the regional norm. On the demographic side, these neighborhoods exhibit substantially lower representation of Black ethnic groups than the regional average. Additionally, the rate of residents who cannot speak English well or at all is considerably below the regional average, indicating better overall English proficiency among the population.\"\n",
    "  },\n",
    "  \"B\": {\n",
    "    \"name\": \"Affluent Rural Suburban\",\n",
    "    \"description\": \"Cluster B neighborhoods are characterized by a significantly higher prevalence of detached houses compared to the regional average, suggesting a more spacious and possibly rural or suburban housing landscape. These areas have notably fewer residents born in EU countries, non-EU European countries, or Africa than the regional norm, pointing to lower levels of international migration. Ethnically, they show substantially reduced proportions of Bangladeshi, Pakistani, other Asian, Black, and mixed or multiple ethnic groups relative to the regional average. The rate of individuals who cannot speak English well or at all is markedly lower than typical, implying stronger English language skills across the population. Religious diversity is less pronounced, with other religions represented at a considerably lower rate than the region. Household structures lean towards fewer one-person households and more families without children, but the standout is a dramatically reduced presence of communal living establishments. Housing types reflect fewer terraced houses, flats, maisonettes, or apartments, as well as substantially lower social and private renting compared to the regional average. Health indicators, such as standardized illness ratios, are better than the regional norm, indicating lower illness rates. Unemployment is also notably below the regional average, contributing to a more stable economic environment.\"\n",
    "  },\n",
    "  \"C\": {\n",
    "    \"name\": \"Diverse Urban Renters\",\n",
    "    \"description\": \"Cluster C neighborhoods stand out with significantly higher proportions of residents born in EU countries, non-EU European countries, and Africa compared to the regional average, reflecting substantial international diversity. Ethnically, these areas have markedly elevated representations of Bangladeshi, Chinese, Indian, Pakistani, other Asian, Black, and mixed or multiple ethnic groups relative to the regional norm, while White ethnic groups are somewhat less prominent. The rate of residents who cannot speak English well or at all is considerably above the regional average, suggesting challenges in language proficiency. Other religions are more common than typical in the region. Housing features a notably higher presence of terraced houses, including end-terraces, and flats, maisonettes, or apartments compared to the average. Tenure shows elevated levels of social renting and private renting beyond the regional norm, with ownership or shared ownership being less prevalent. Vehicle ownership is lower, with fewer households having two or more cars or vans than the regional average. Unemployment rates are substantially higher than the region, indicating potential economic pressures. Additionally, the proportion of very elderly residents is below the regional average, contributing to a younger demographic tilt.\"\n",
    "  },\n",
    "  \"D\": {\n",
    "    \"name\": \"Working-Class Terraced Renters\",\n",
    "    \"description\": \"Cluster D neighborhoods exhibit significantly lower proportions of residents born in non-EU European countries and Africa compared to the regional average, indicating reduced international origins from these areas. Ethnically, representations of Bangladeshi, Chinese, Indian, Pakistani, and other Asian groups are markedly below the regional norm. Living arrangements show fewer individuals in communal establishments than typical. Housing types feature a notably higher presence of terraced houses, including end-terraces, relative to the average, while detached houses or bungalows are less common. Social renting is elevated beyond the regional norm. Health indicators like standardized illness ratios are somewhat higher than average, suggesting more health challenges, though not drastically so. Unemployment is also above the regional average, pointing to economic vulnerabilities.\"\n",
    "  },\n",
    "  \"E\": {\n",
    "    \"name\": \"Transient Cosmopolitan Students\",\n",
    "    \"description\": \"Cluster E neighborhoods are distinguished by substantially higher proportions of residents born in EU countries, non-EU European countries, and Africa compared to the regional average, highlighting intense global diversity. Ethnically, these areas feature markedly elevated representations of Bangladeshi, Chinese, Indian, Pakistani, other Asian, Black, and mixed or multiple ethnic groups relative to the norm. The rate of individuals who cannot speak English well or at all is significantly above average, indicating notable language barriers. Religiously, no religion is more prevalent, while Christianity is considerably less common, and other religions are much more represented than in the region. Marital status shows higher rates of never married individuals and lower rates of married or civil partnered residents, as well as fewer separated or divorced. Households have more one-person setups and fewer with dependent children compared to the average, with substantially reduced instances of all members sharing the same ethnic group. Communal establishments are dramatically more common, and residential stability is lower, with fewer people living at the same address as the previous year. Housing leans heavily towards flats, maisonettes, or apartments at rates far above the regional norm, while detached, semi-detached, and terraced houses are all significantly less prevalent. Ownership is below average, but private renting is notably higher. Demographically, proportions of young children, school-aged children, middle-aged adults, and elderly residents are all markedly lower than the region, creating a younger adult focus. Provision of unpaid care is less common. Vehicle ownership is reduced, with fewer households having two or more cars. Educationally, highest qualifications at level 1-2 or apprenticeship are below average, but level 4 or above are higher, alongside a substantially elevated full-time student population. Occupationally, skilled trades and process plant/machine operatives are less represented than the regional average.\"\n",
    "  }\n",
    "}\n",
    "cluster_descriptions_df = pd.DataFrame.from_dict(cluster_summaries, orient='index')\n",
    "\n",
    "\n",
    "#pretty print the descriptions (break lines for readability)\n",
    "for cluster, row in cluster_descriptions_df.iterrows():\n",
    "    print(f\"Cluster {cluster} - {row['name']}:\\n\")\n",
    "    description = row['description']\n",
    "    #break into lines of max 80 characters\n",
    "    import textwrap\n",
    "    wrapped_description = textwrap.fill(description, width=70)\n",
    "    print(wrapped_description, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results\n",
    "Lets save the results to file for use in GIS software or to format for sharing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Subclustering\n",
    "\n",
    "We often want to perform a finer level of clustering to capture more detailed patterns in the data.\n",
    "For OAC the top level \"supergroup\" clusters are split further into groups and subgroups by applying the above process iteratively. This process is referred to as top down clustering. This has the advantage of allowing more clusters to be created without needing to consider all clusters at once. It also allows for more interpretable clusters as the subclusters are nested within the broader supergroup clusters.\n",
    "\n",
    "## Selecting the number of subclusters for each supergroup using Clustergrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subcluster_clustergrams(output_df, num_clusters, n_init=1):\n",
    "    \"\"\"\n",
    "    Generate and save clustergrams for each supercluster.\n",
    "    This function loops through the existing clusters and creates a clustergram \n",
    "    for each\n",
    "    Parameters:\n",
    "    output_df (pd.DataFrame): DataFrame containing cluster assignments.\n",
    "    num_clusters (int): The total number of clusters to iterate over.\n",
    "    n_init (int, optional): Number of times K-means runs with different centroid seeds.\n",
    "                            Defaults to 1 for quick testing.\n",
    "\n",
    "    \"\"\"\n",
    "    save_dir = \"outputs/plots\" #directory to save the clustergrams\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure save directory exists\n",
    "    cluster_labels = np.sort(output_df[\"cluster\"].unique())\n",
    "    print(cluster_labels)\n",
    "    for cluster_label in cluster_labels:\n",
    "        # Select rows corresponding to the current cluster, dropping the 'cluster' column\n",
    "        cluster_df = output_df.query(\"cluster == @cluster_label\").drop(columns='cluster')\n",
    "\n",
    "        print(f\"Cluster: {cluster_label,cluster_summaries[cluster_label][\"name\"]}, {len(cluster_df)} geographies in cluster\")\n",
    "\n",
    "        if cluster_df.empty:\n",
    "            print(f\"Skipping cluster {cluster_label} as it has no data.\")\n",
    "            continue\n",
    "\n",
    "        # Define save location\n",
    "        save_loc = os.path.join(save_dir, f\"subcluster_clustergram_cluster{cluster_label}.png\")\n",
    "        print(f\"Saving clustergram to {save_loc}\")\n",
    "\n",
    "        # Generate clustergram\n",
    "        create_clustergram(cluster_df, n_init=n_init, save_loc=save_loc)\n",
    "\n",
    "# Example usage\n",
    "create_subcluster_clustergrams(supergrouped_variable_df, num_clusters, n_init=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the subclustering \n",
    "We can now select the number of subclusters to split each of the supergroups into using the clustergrams above.\n",
    "The length of the list must match num_clusters (the number of supergroups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcluster_nums = [2, 3, 3, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_subclustering(input_df: pd.DataFrame, subcluster_nums: list, num_clusters: int, n_init: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs subclustering for each supergroup using KMeans and returns a modified DataFrame with subcluster labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - output_df (pd.DataFrame): The original DataFrame containing data and cluster assignments.\n",
    "    - subcluster_nums (list): A list specifying the number of subclusters to split each supergroup into.\n",
    "    - num_clusters (int): The total number of supergroups.\n",
    "    - n_init (int, optional): The number of times KMeans will be initialized. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A new the output dataFrame with an added 'subcluster' column.\n",
    "    \"\"\"\n",
    "\n",
    "    cluster_labels = np.sort(input_df[\"cluster\"].unique())\n",
    "    print(f\"Cluster labels found: {cluster_labels}\")\n",
    "    if len(subcluster_nums) != len(cluster_labels):\n",
    "        raise ValueError(f\"Length of subcluster_nums ({len(subcluster_nums)}) does not match num_clusters ({len(cluster_labels)}).\")\n",
    "\n",
    "    # Work on a copy of the DataFrame to prevent unintended modifications\n",
    "    df = input_df.copy()\n",
    "\n",
    "    for cluster, num_subclusters in zip(cluster_labels, subcluster_nums):\n",
    "        print(f\"Clustering supergroup {cluster,cluster_summaries[cluster][\"name\"]} into {num_subclusters} subclusters.\")\n",
    "\n",
    "        # Select rows corresponding to the current cluster, drop the cluster column before clustering\n",
    "        cluster_df = input_df.query(\"cluster == @cluster\").drop(columns='cluster').copy()\n",
    "        # Run KMeans clustering for the selected supergroup\n",
    "        subcluster_output_df = run_kmeans(\n",
    "            cluster_df, \n",
    "            num_subclusters, \n",
    "            n_init=n_init, \n",
    "            output_filepath=f\"outputs/subclusters/supergroup{cluster}_subclusteroutput.csv\"\n",
    "        )\n",
    "\n",
    "        # Combine anems\n",
    "        subcluster_output_df['subcluster'] = [str(cluster) + str(i) for i in subcluster_output_df['cluster']]\n",
    "\n",
    "        # Update the modified DataFrame with subclustering results\n",
    "        df.loc[cluster_df.index, 'subcluster'] = subcluster_output_df['subcluster']\n",
    "\n",
    "    # Save the final output\n",
    "    df.to_csv(\"outputs/subgroups_clusteroutput.csv\")\n",
    "    print(\"Final output saved to outputs/subgroups_clusteroutput.csv\")\n",
    "\n",
    "    return df  # Return the modified DataFrame with clusters and subclusters\n",
    "subgrouped_variable_df = run_subclustering(supergrouped_variable_df, subcluster_nums, num_clusters=num_clusters, n_init=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate percentage difference (subclusters vs cluster means) ---\n",
    "\n",
    "# cluster means for reference\n",
    "cluster_means = subgrouped_variable_df.groupby(\"cluster\").mean(numeric_only=True)\n",
    "global_means = subgrouped_variable_df[features].mean()\n",
    "# subcluster means\n",
    "subcluster_means = subgrouped_variable_df.groupby([\"cluster\", \"subcluster\"]).mean(numeric_only=True)\n",
    "\n",
    "# percentage difference: subcluster relative to parent cluster\n",
    "pct_diff_sub = (subcluster_means / cluster_means) * 100\n",
    "pct_diff_sub = (subcluster_means/ global_means)*100\n",
    "pct_display_df_sub = pct_diff_sub.T  # index = features (encodings), columns = MultiIndex (cluster, subcluster)\n",
    "#replace the column MultiIndex with a single level index with \"cluster-subcluster\" format and swap in the cluster names from cluster_summaries\n",
    "# pct_display_df_sub.columns = [f\"{c[0]}-{c[1]}\" for c in pct_display_df_sub.columns]\n",
    "pct_display_df_sub.columns = [f\"{cluster_summaries[c[0]]['name']}-{c[1]}\" for c in pct_display_df_sub.columns]\n",
    "\n",
    "# pct_display_df_sub.columns = [f\"{c[0]}-{c[1]}\" for c in pct_display_df_sub.columns]\n",
    "\n",
    "# build customdata for hover (human names repeated across cluster–subcluster combos)\n",
    "human_names = pct_display_df_sub.index.map(lambda e: encoding_to_name.get(e, e)).values\n",
    "customdata_pct_sub = np.tile(human_names.reshape(-1, 1), (1, pct_display_df_sub.shape[1]))\n",
    "\n",
    "# get symmetric range around 100\n",
    "max_abs_sub = np.nanmax(np.abs(pct_display_df_sub.values - 100))\n",
    "\n",
    "# --- Heatmap (percentage difference: subcluster vs cluster mean) ---\n",
    "fig_pct_sub = px.imshow(\n",
    "    pct_display_df_sub,\n",
    "    color_continuous_scale=\"RdYlGn\",\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    labels=dict(x=\"Subcluster\", y=\"Feature (encoding)\", color=\"% of cluster mean\"),\n",
    "    zmin=0,\n",
    "    zmax=200\n",
    ")\n",
    "\n",
    "# attach customdata and set hover\n",
    "fig_pct_sub.data[0].customdata = customdata_pct_sub\n",
    "fig_pct_sub.update_traces(\n",
    "    hovertemplate=\"Subcluster: %{x}<br>Encoding: %{y}<br>Name: %{customdata}<br>% of Cluster Mean: %{z:.1f}%<extra></extra>\",\n",
    "    zmid=100  # centre colours on 100%\n",
    ")\n",
    "\n",
    "fig_pct_sub.update_layout(\n",
    "    title=\"Subcluster Profiles (% of Cluster Mean)\",\n",
    "    xaxis_title=\"Subcluster\",\n",
    "    yaxis_title=\"Feature (encoding)\",\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# get mapping of column → cluster\n",
    "col_clusters = [col.split(\"-\")[0] for col in pct_display_df_sub.columns]\n",
    "\n",
    "# find where cluster changes (between adjacent columns)\n",
    "boundaries = [\n",
    "    i + 0.5 for i in range(len(col_clusters) - 1)\n",
    "    if col_clusters[i] != col_clusters[i + 1]\n",
    "]\n",
    "\n",
    "# add vertical lines at these boundaries\n",
    "for b in boundaries:\n",
    "    fig_pct_sub.add_vline(\n",
    "        x=b, line_width=2, line_dash=\"dash\", line_color=\"black\"\n",
    "    )\n",
    "\n",
    "fig_pct_sub.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results to file for use in GIS software or to format for sharing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the geometry column from oas_region to subgrouped_variable_df to make a geodataframe\n",
    "gdf = oas_region.merge(subgrouped_variable_df  , left_index=True, right_index=True, how='left')\n",
    "#save to file\n",
    "gdf.to_file(\"outputs/subclusters_geodataframe.gpkg\", layer='subclusters', driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the subclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "#| echo: false\n",
    "\n",
    "# Custom cluster colours for subclusters\n",
    "def generate_shades(hex_color, n_shades=5, step=0.15):\n",
    "    \"\"\"Generate shades centred around a base color.\"\"\"\n",
    "    base = np.array(mcolors.to_rgb(hex_color))\n",
    "    mid = n_shades // 2\n",
    "    shades = []\n",
    "    for i in range(n_shades):\n",
    "        factor = 1 + (i - mid) * step\n",
    "        shade = np.clip(base * factor, 0, 1)\n",
    "        shades.append(mcolors.to_hex(shade))\n",
    "    return shades\n",
    "\n",
    "#same as before \n",
    "base_colors = {\n",
    "    \"A\": '#8dd3c7',\n",
    "    \"B\": '#ffffb3',\n",
    "    \"C\": '#bebada',\n",
    "    \"D\": '#fb8072',\n",
    "    \"E\": '#fdb462',\n",
    "    \"F\": \"#235477\",\n",
    "    \"G\": '#fccde5',\n",
    "    \"H\": '#d9d9d9',\n",
    "    \"I\": '#bc80bd',\n",
    "    \"J\": '#ccebc5'\n",
    "}\n",
    "\n",
    "# Generate subgroup colours\n",
    "colors = {}\n",
    "for (group, hex_color), n_sub in zip(base_colors.items(), subcluster_nums):\n",
    "    shades = generate_shades(hex_color, n_shades=n_sub)\n",
    "    for i, shade in enumerate(shades, start=1):\n",
    "        colors[f\"{group}{i}\"] = shade\n",
    "\n",
    "\n",
    "sorted_clusters = sorted(colors.keys())\n",
    "color_list = [colors[k] for k in sorted_clusters]\n",
    "\n",
    "# # Ensure cluster column is string for Kepler\n",
    "# supergrouped_variable_df[\"cluster\"] = supergrouped_variable_df[\"cluster\"].astype(str)\n",
    "# #append geometry column from oas_liv to supergrouped_variable_df to make a geodataframe\n",
    "gdf = oas_region.merge(subgrouped_variable_df  , left_index=True, right_index=True, how='left')\n",
    "\n",
    "\n",
    "# Config for polygons (fill only, no stroke)\n",
    "config = {\n",
    "    \"version\": \"v1\",\n",
    "    \"config\": {\n",
    "        \"visState\": {\n",
    "            \"filters\": [],\n",
    "            \"layers\": [\n",
    "                {\n",
    "                    \"id\": \"clusters_layer\",\n",
    "                    \"type\": \"geojson\",   # polygon layer\n",
    "                    \"config\": {\n",
    "                        \"dataId\": \"clusters\",\n",
    "                        \"label\": \"Clusters\",\n",
    "                        'columns': {'geojson': 'geometry'},\n",
    "                        \"color\": [130, 154, 227],\n",
    "                        \"highlightColor\": [252, 242, 26, 255],\n",
    "                        \"isVisible\": True,\n",
    "                        \"visConfig\": {\n",
    "                            \"opacity\": 0.8,\n",
    "                            \"thickness\": 0,\n",
    "                            \"strokeColor\": None,\n",
    "                            \"colorRange\": {\n",
    "                                \"name\": \"Custom\",\n",
    "                                \"type\": \"qualitative\",\n",
    "                                \"category\": \"Custom\",\n",
    "                                \"colors\": color_list\n",
    "                            },\n",
    "                            \"filled\": True\n",
    "                        },\n",
    "                        \"hidden\": False,\n",
    "                        \"textLabel\": []\n",
    "                    },\n",
    "                    \"visualChannels\": {\n",
    "                        \"colorField\": {\"name\": \"subcluster\", \"type\": \"string\"},\n",
    "                        \"colorScale\": \"ordinal\",\n",
    "                        \"strokeColorField\": None,\n",
    "                        \"strokeColorScale\": \"quantile\",\n",
    "                        \"sizeField\": None,\n",
    "                        \"sizeScale\": \"linear\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"effects\": [],\n",
    "            \"interactionConfig\": {\n",
    "                \"tooltip\": {\n",
    "                    \"fieldsToShow\": {\n",
    "                        \"clusters\": [\n",
    "                            {\"name\": \"subcluster\", \"format\": None}\n",
    "                        ]\n",
    "                    },\n",
    "                    \"enabled\": True\n",
    "                }\n",
    "            },\n",
    "            \"layerBlending\": \"normal\"\n",
    "        },\n",
    "        \"mapState\": {\n",
    "            \"bearing\": 0,\n",
    "            \"dragRotate\": False,\n",
    "               'latitude': centroid_lat,\n",
    "                'longitude': centroid_lon,\n",
    "            \"pitch\": 0,\n",
    "            \"zoom\": 9,\n",
    "            \"isSplit\": False\n",
    "        },\n",
    "        \"mapStyle\": {\n",
    "            \"styleType\": \"dark-matter\",\n",
    "            \"topLayerGroups\": {\n",
    "                \"water\": True,\n",
    "                \"building\": True\n",
    "            },\n",
    "            \"visibleLayerGroups\": {\n",
    "                \"label\": True,\n",
    "                \"road\": True,\n",
    "                \"border\": False,\n",
    "                \"building\": True,\n",
    "                \"water\": True,\n",
    "                \"land\": True,\n",
    "                \"3d building\": False\n",
    "            },\n",
    "            \"mapStyles\": {\n",
    "                \"dark-matter\": {\n",
    "                    \"id\": \"dark-matter\",\n",
    "                    \"label\": \"Carto Dark Matter\",\n",
    "                    \"url\": \"https://basemaps.cartocdn.com/gl/dark-matter-gl-style/style.json\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Show in Kepler\n",
    "map_with_basemap = KeplerGl(data={\"clusters\":gdf}, config=config, height=600)\n",
    "map_with_basemap.save_to_html(file_name=\"outputs/maps/subcluster_map.html\")\n",
    "\n",
    "map_with_basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"outputs/maps/subcluster_map.html\" width=\"100%\" height=\"650\" style=\"border:none;\"></iframe>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
